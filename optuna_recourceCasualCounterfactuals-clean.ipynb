{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env LD_LIBRARY_PATH=$HOME/cfcx/recourse/model/LORE/yadt:$LD_LIBRARY_PATH\n",
    "    \n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import make_dot\n",
    "import pickle\n",
    "from lux.lux import LUX\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latex(aggregated, aggregated_std, index_name='', caption='',label=''):\n",
    "    aggregated=aggregated.replace('_','\\_')\n",
    "    aggregated_std=aggregated_std.replace('_','\\_')\n",
    "    print('\\\\begin{table}')\n",
    "    print('\\\\caption{'+caption+'}')\n",
    "    print('\\\\label{'+label+'}')\n",
    "    print('\\\\begin{tabularx}{\\\\textwidth}{|X|'+'|'.join(['X']*len(aggregated.columns))+'|}')\n",
    "    print('\\\\hline')\n",
    "    print(index_name+' & '+'&'.join(aggregated.columns))\n",
    "    print('\\\\\\\\ \\\\hline \\\\hline')\n",
    "    for i,row in aggregated.iterrows():\n",
    "        row_std = aggregated_std.loc[i]\n",
    "        rowstring = []\n",
    "        for r,r_std in zip(row,row_std):\n",
    "            rowstring.append(\"{:0.2f}\".format(r)+' $\\pm$ '+\"{:0.2f}\".format(r_std))\n",
    "        print(str(i)+' & '+' & '.join(rowstring)+'\\\\\\\\ \\hline')\n",
    "    print('\\\\end{tabularx}\\n\\\\end{table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latex_nostd(aggregated, index_name='', caption='',label=''):\n",
    "    aggregated=aggregated.replace('_','\\_')\n",
    "    print('\\\\begin{table}')\n",
    "    print('\\\\caption{'+caption+'}')\n",
    "    print('\\\\label{'+label+'}')\n",
    "    print('\\\\begin{tabularx}{\\\\textwidth}{|X|'+'|'.join(['X']*len(aggregated.columns))+'|}')\n",
    "    print('\\\\hline')\n",
    "    print(index_name+' & '+'&'.join(aggregated.columns))\n",
    "    print('\\\\\\\\ \\\\hline \\\\hline')\n",
    "    for i,row in aggregated.iterrows():\n",
    "        rowstring = []\n",
    "        for r in row:\n",
    "            rowstring.append(\"{:0.2f}\".format(r))\n",
    "        print(str(i)+' & '+' & '.join(rowstring)+'\\\\\\\\ \\hline')\n",
    "    print('\\\\end{tabularx}\\n\\\\end{table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import softmax\n",
    "class EBMCounterOptimizer:\n",
    "    Y_TEST = '_eco_y_test'\n",
    "    Y_PRED = '_eco_y_pred'\n",
    "    IS_MODIFIABLE = '_eco_is_modifiable'\n",
    "\n",
    "    def __init__(self, ebm, X):\n",
    "        self.ebm = ebm\n",
    "        self.X = X\n",
    "        self.updated_features = {}\n",
    "        \n",
    "    def _get_optimized_feature_value(self, feature_name, feature_idx, feature_val, features, feature_masked, term_idx, class_idx):\n",
    "        \"\"\"\n",
    "        Returns feature value with maximum score for given target class. \n",
    "        \n",
    "        @Todo Needs changes to return optimized value due to given strategy. \n",
    "        \"\"\"\n",
    "\n",
    "        if feature_name in feature_masked and feature_name not in self.updated_features:\n",
    "            if len(self.ebm.term_scores_[term_idx].shape) > 1:\n",
    "                class_term_scores = self.ebm.term_scores_[term_idx].T[class_idx]\n",
    "            else:\n",
    "                class_term_scores =  self.ebm.term_scores_[term_idx] if class_idx == 1 else 1-self.ebm.term_scores_[term_idx]\n",
    "            class_max = np.max(class_term_scores)\n",
    "            feature_score_idx = np.where(class_term_scores[1:-1]==class_max)[0][0] ##this is score, not value imho\n",
    "            # we bin differently for main effects and pairs, so first \n",
    "            # get the list containing the bins for different resolutions\n",
    "            bin_levels = self.ebm.bins_[feature_idx]\n",
    "            #print(f'Feature score index for feature {feature_name} is {feature_score_idx} which represents score equal: {class_max} test: {class_term_scores[feature_score_idx+1]}')\n",
    "            # what resolution do we need for this term (main resolution, pair\n",
    "            # resolution, etc.), but limit to the last resolution available\n",
    "            bins = bin_levels[min(len(bin_levels), len(features)) - 1]\n",
    "            \n",
    "            if len(bins)==0:\n",
    "                feature_val = self.X[feature_name].sample(1).values[0]\n",
    "            else:\n",
    "                if isinstance(bins, dict):\n",
    "                    # categorical feature\n",
    "                    # 'unknown' category strings are in the last bin (-1)\n",
    "                    feature_val=list(bins.values())[feature_score_idx-1] # if maxscore was 0, or -1 just assign random value\n",
    "                else:\n",
    "                    # continuous feature\n",
    "                    # Get the lower and upper bounds of the specified bin\n",
    "                    lower_idx = feature_score_idx-1\n",
    "                    upper_idx = feature_score_idx\n",
    "                    \n",
    "                    if lower_idx == -1:\n",
    "                        lower = self.ebm.feature_bounds_[feature_idx][0]\n",
    "                    else:\n",
    "                        lower = bins[lower_idx]\n",
    "\n",
    "                    if upper_idx == len(bins):\n",
    "                        upper = self.ebm.feature_bounds_[feature_idx][1]\n",
    "                    else:\n",
    "                        upper = bins[upper_idx]\n",
    "                    #print(f'Drawing randomly from :{lower} to {upper}')\n",
    "\n",
    "                    # Draw a random number from the range defined by the bin\n",
    "                    feature_val = np.random.uniform(lower, upper)\n",
    "\n",
    "            #print(f'This translates into feature value: {feature_val}')\n",
    "            \n",
    "            self.updated_features.update({feature_name: feature_val})\n",
    "        elif feature_name in self.updated_features:\n",
    "            feature_val = self.updated_features.get(feature_name)\n",
    "        else:\n",
    "            self.updated_features.update({feature_name: feature_val})\n",
    "\n",
    "        return feature_val\n",
    "        \n",
    "\n",
    "    def optimize_proba(self, target_class, feature_masked):\n",
    "        \"\"\"\n",
    "        The method calculates probabilities taking into account the optimization of given parameters towards the target class.\n",
    "        Method is based on a default ebm's predict_proba\n",
    "\n",
    "        Parameters:\n",
    "        ebm: Trained EBM model\n",
    "        X: Dataset\n",
    "        target_class: Target class from which we take the features\n",
    "        featured_masked: List of interchangeable features\n",
    "        \"\"\"\n",
    "        if target_class not in self.ebm.classes_:\n",
    "            raise KeyError(f'Class \"{target_class}\" does not exists in given EBM model')\n",
    "\n",
    "        class_idx = np.where(self.ebm.classes_==target_class)[0][0]\n",
    "        self.updated_features = {}\n",
    "        sample_scores = []\n",
    "        cf = {}\n",
    "        for index, sample in self.X.iterrows():\n",
    "            # start from the intercept for each sample\n",
    "            score = self.ebm.intercept_.copy()\n",
    "            if isinstance(score, float) or len(score) == 1:\n",
    "                # regression or binary classification\n",
    "                score = float(score)\n",
    "\n",
    "            # we have 2 terms, so add their score contributions\n",
    "            for term_idx, features in enumerate(self.ebm.term_features_):\n",
    "                # indexing into a tensor requires a multi-dimensional index\n",
    "                tensor_index = []\n",
    "                # main effects will have 1 feature, and pairs will have 2 features\n",
    "                for feature_idx in features:\n",
    "                    feature_name = self.ebm.feature_names_in_[feature_idx]  # Get the feature name by index\n",
    "                    feature_val = sample[feature_name]  # Use the feature name to get the correct value from the sample\n",
    "                    bin_idx = 0  # if missing value, use bin index 0\n",
    "\n",
    "                    if feature_val is not None and feature_val is not np.nan:\n",
    "                        # we bin differently for main effects and pairs, so first \n",
    "                        # get the list containing the bins for different resolutions\n",
    "                        bin_levels = self.ebm.bins_[feature_idx]\n",
    "\n",
    "                        # what resolution do we need for this term (main resolution, pair\n",
    "                        # resolution, etc.), but limit to the last resolution available\n",
    "                        bins = bin_levels[min(len(bin_levels), len(features)) - 1]\n",
    "\n",
    "                        \n",
    "                        # here is where the magic is located\n",
    "                        feature_val = self._get_optimized_feature_value(feature_name, feature_idx, feature_val, features, feature_masked, term_idx, class_idx)\n",
    "\n",
    "                        if isinstance(bins, dict):\n",
    "                            # categorical feature\n",
    "                            # 'unknown' category strings are in the last bin (-1)\n",
    "                            bin_idx = bins.get(feature_val, -1)\n",
    "                            if bin_idx == -1:\n",
    "                                # check value as string\n",
    "                                bin_idx = bins.get(str(feature_val), -1)\n",
    "                        else:\n",
    "                            # continuous feature\n",
    "                            try:\n",
    "                                # try converting to a float, if that fails it's 'unknown'\n",
    "                                feature_val = float(feature_val)\n",
    "                                # add 1 because the 0th bin is reserved for 'missing'\n",
    "                                bin_idx = np.digitize(feature_val, bins) + 1\n",
    "                            except ValueError:\n",
    "                                # non-floats are 'unknown', which is in the last bin (-1)\n",
    "                                bin_idx = -1\n",
    "                                \n",
    "                        if len(self.ebm.term_scores_[term_idx].shape) > 1:\n",
    "                            sc = self.ebm.term_scores_[term_idx].T[class_idx][bin_idx]\n",
    "                        else:\n",
    "                            sc = self.ebm.term_scores_[term_idx][bin_idx]\n",
    "                        #print(f'And feature value {feature_val} translates back to bin index: {bin_idx} which represents score: {sc}')\n",
    "                        \n",
    "                        tensor_index.append(bin_idx)\n",
    "                \n",
    "\n",
    "                # local_score is also the local feature importance\n",
    "                local_score = self.ebm.term_scores_[term_idx][tuple(tensor_index)]\n",
    "\n",
    "                score += local_score\n",
    "            sample_scores.append(score)\n",
    "\n",
    "        predictions = np.array(sample_scores)\n",
    "\n",
    "        if hasattr(self.ebm, 'classes_'):\n",
    "            # classification\n",
    "            if len(self.ebm.classes_) == 2:\n",
    "                # binary classification\n",
    "\n",
    "                # softmax expects two logits for binary classification\n",
    "                # the first logit is always equivalent to 0 for binary classification\n",
    "                predictions = [[0, x] for x in predictions]\n",
    "            predictions = softmax(predictions)\n",
    "\n",
    "        return predictions,self.updated_features\n",
    "    \n",
    "    def check_samples(self, target_class, y_test_key, feature_masked):\n",
    "        X = self.X.copy()\n",
    "        predictions = self.optimize_proba(target_class, feature_masked)\n",
    "        X.loc[:, self.Y_TEST] = np.argmax(predictions, axis=1)\n",
    "        X.loc[:, self.Y_PRED] = X[self.Y_TEST].map({key:val for key, val in enumerate(self.ebm.classes_)})\n",
    "        X.loc[:, self.IS_MODIFIABLE] = np.where(X[y_test_key]!=X[self.Y_PRED], 1, 0)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_from_csf(adjacency_matrix, csf, sample_order=None, categorical=None):\n",
    "    num_vars = adjacency_matrix.shape[0]\n",
    "    samples = np.zeros((1, num_vars))\n",
    "    \n",
    "    if categorical is None:\n",
    "        categorical = [False] * num_vars  # Default to all variables being non-categorical\n",
    "\n",
    "    if sample_order is None:\n",
    "        sample_order = range(num_vars)\n",
    "\n",
    "    # Assign noise values to variables without parents\n",
    "\n",
    "    for j in range(num_vars):\n",
    "        if np.all(adjacency_matrix[j, :] == 0):  # If no parents, assign noise\n",
    "            samples[0, j] = csf[j]\n",
    "            if categorical[j]:  # Round the value if the variable is categorical\n",
    "                samples[0, j] = np.round(samples[0, j])\n",
    "\n",
    "\n",
    "    # Generate samples based on the specified order\n",
    "    for j in sample_order:\n",
    "        parents = np.where(adjacency_matrix[j, :] != 0)[0]\n",
    "        if len(parents) > 0:\n",
    "            # Predicted values based on parents\n",
    "            predicted_values = np.dot(samples[:, parents], adjacency_matrix[j, parents])\n",
    "            samples[:, j]=predicted_values\n",
    "            if categorical[j]:  # Round the value if the variable is categorical\n",
    "                samples[:, j] = np.round(samples[:, j])\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents(adjacency_matrix):\n",
    "    parents = []\n",
    "    num_vars = adjacency_matrix.shape[0]\n",
    "    for j in range(num_vars):\n",
    "        if np.all(adjacency_matrix[j, :] == 0):\n",
    "            parents.append(j)\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(adjacency_matrix, num_samples, bounds = None,  sample_order=None, categorical=None):\n",
    "    num_vars = adjacency_matrix.shape[0]\n",
    "    samples = np.zeros((num_samples, num_vars))\n",
    "    \n",
    "    if categorical is None:\n",
    "        categorical = [False] * num_vars  # Default to all variables being non-categorical\n",
    "\n",
    "    if sample_order is None:\n",
    "        sample_order = range(num_vars)\n",
    "\n",
    "    # Assign noise values to variables without parents\n",
    "    for i in range(num_samples):\n",
    "        if bounds is None:\n",
    "            noise = np.random.normal(size=num_vars) #assign_csf\n",
    "        else:\n",
    "            noise = np.random.normal(size=num_vars)\n",
    "        for j in range(num_vars):\n",
    "            if np.all(adjacency_matrix[j, :] == 0):  # If no parents, assign noise\n",
    "                samples[i, j] = noise[j]\n",
    "                if categorical[j]:  # Round the value if the variable is categorical\n",
    "                    samples[i, j] = np.round(samples[i, j])\n",
    "\n",
    "\n",
    "    # Generate samples based on the specified order\n",
    "    for j in sample_order:\n",
    "        parents = np.where(adjacency_matrix[j, :] != 0)[0]\n",
    "        if len(parents) > 0:\n",
    "            # Predicted values based on parents\n",
    "            predicted_values = np.dot(samples[:, parents], adjacency_matrix[j, parents])\n",
    "            samples[:, j]=predicted_values\n",
    "            if categorical[j]:  # Round the value if the variable is categorical\n",
    "                samples[:, j] = np.round(samples[:, j])\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: in such calcualtion , the difference in cat feature should always be equal to 1 as a penalty\n",
    "def compute_causal_penalty(samples, adjacency_matrix, sample_order, categorical=None):\n",
    "    \"\"\"\n",
    "    Calculate the inconsistency of samples with the given adjacency matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - adjacency_matrix: np.ndarray, the adjacency matrix from DirectLiNGAM\n",
    "    - samples: np.ndarray, the samples to evaluate (num_samples x num_features)\n",
    "    \n",
    "    Returns:\n",
    "    - inconsistency: float, a measure of how inconsistent the samples are with the adjacency matrix\n",
    "    \"\"\"\n",
    "    num_samples, num_features = samples.shape\n",
    "    inconsistency = 0.0\n",
    "\n",
    "    if categorical is None:\n",
    "        categorical = [False]*len(sample_order)\n",
    "    # Iterate through each feature and its causal parents\n",
    "    for i in sample_order:\n",
    "        parents = np.where(adjacency_matrix[i, :] != 0)[0]\n",
    "        if len(parents) > 0:\n",
    "            # Predicted values based on parents\n",
    "            predicted_values = np.dot(samples[:, parents], adjacency_matrix[i, parents])\n",
    "            # Calculate the inconsistency as the mean squared error\n",
    "            mse = (samples[:, i] - predicted_values) ** 2\n",
    "            if categorical[i]:\n",
    "                mse = min((1,mse))\n",
    "            inconsistency += mse\n",
    "\n",
    "    return np.sqrt(np.mean(inconsistency))/len(sample_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_yloss(model, cfs, desired_class):\n",
    "    predicted_value = np.array(model.predict_proba(cfs))\n",
    "    maxvalue = np.full((len(predicted_value)), -np.inf)\n",
    "    yloss=0\n",
    "    for c in range(len(model.classes_)):\n",
    "        #print(c)\n",
    "        if c != desired_class:\n",
    "            maxvalue = np.maximum(maxvalue, predicted_value[:, c])\n",
    "    #print(predicted_value)\n",
    "    #print(maxvalue)\n",
    "    yloss = np.maximum(0, maxvalue - predicted_value[:, int(desired_class)])\n",
    "    return yloss,maxvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data has to be normalized and proximity will be consine similarity in that casse\n",
    "\n",
    "def compute_proximity_loss(cfs, query_instance, pbounds, features_order = None, feature_weights=None, categorical=None):\n",
    "    \"\"\"Compute weighted distance between two vectors.\"\"\"\n",
    "    # If feature_weights is None, assign an array of ones with the size of cfs.shape[1]\n",
    "    \n",
    "    if feature_weights is None:\n",
    "        feature_weights = np.ones(cfs.shape[1])\n",
    "    if categorical is None:\n",
    "        categorical = [False]*cfs.shape[1]\n",
    "    \n",
    "    diff = abs(cfs - query_instance)\n",
    "    diff=[1 if categorical[i] and v > 0 else v for i, v in enumerate(diff[0, :])]\n",
    "    \n",
    "    diff=[v/(pbounds[features_order[i]][1]-pbounds[features_order[i]][0]) if not categorical[i] and features_order[i] in pbounds.keys() else v for i, v in enumerate(diff)]\n",
    "    product = np.multiply(\n",
    "        (diff),\n",
    "        feature_weights)\n",
    "    product = product.reshape(-1, product.shape[-1])\n",
    "    proximity_loss = np.sum(product, axis=1)\n",
    "\n",
    "    # Dividing by the sum of feature weights to normalize proximity loss\n",
    "    return proximity_loss / sum(feature_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity_loss(cfs,query_instance):\n",
    "        \"\"\"Compute weighted distance between two vectors.\"\"\"\n",
    "        sparsity_loss = np.count_nonzero(cfs - query_instance, axis=1)\n",
    "        return sparsity_loss / cfs.shape[1]  # Dividing by the number of features to normalize sparsity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist,squareform\n",
    "def compute_diversity_loss(cfs,low=1e-6, high=1e-5):\n",
    "    # Compute pairwise distances\n",
    "    pairwise_distances = pdist(cfs)\n",
    "\n",
    "    # Convert the distances to a square matrix form\n",
    "    #TODO: gower distance?\n",
    "    distance_matrix = squareform(pairwise_distances)\n",
    "    \n",
    "    perturbations = np.random.uniform(low=low, high=high, size=distance_matrix.shape[0])\n",
    "\n",
    "    # Add the random perturbations to the diagonal elements of the transformed matrix\n",
    "    np.fill_diagonal(distance_matrix, distance_matrix.diagonal() + perturbations)\n",
    "    \n",
    "    transformed_matrix = 1 / (1 + distance_matrix)\n",
    "        \n",
    "    # Calculate the determinant of the transformed matrix\n",
    "    determinant = np.linalg.det(transformed_matrix) \n",
    "    return determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, cfs, query_instance, desired_class, adjency_matrix, causal_order, \n",
    "                 proximity_weight, sparsity_weight,plausability_weight,diversity_weight, pbounds,features_order,\n",
    "                 masked_features, categorical=None, allcfs=[]):\n",
    "    \"\"\"Computes the overall loss\"\"\"\n",
    "    yloss,maxyloss = compute_yloss(model, cfs, desired_class)\n",
    "    conditional_term = 1.0/(yloss+1+sum([proximity_weight, sparsity_weight,plausability_weight,diversity_weight])) \n",
    "    \n",
    "    proximity_loss = compute_proximity_loss(cfs, query_instance,pbounds,features_order=features_order, categorical=categorical) \\\n",
    "        if proximity_weight > 0 else 0.0\n",
    "    sparsity_loss = compute_sparsity_loss(cfs,query_instance) if sparsity_weight > 0 else 0.0\n",
    "    plausability_loss = compute_causal_penalty(cfs,adjency_matrix,causal_order,categorical=categorical) if plausability_weight > 0 else 0\n",
    "    diversity_loss =1-np.abs(compute_diversity_loss(allcfs)) if len(allcfs) > 1 and diversity_weight > 0 else 0\n",
    "    #print(f'DL: {diversity_loss} for len of cfs {len(allcfs)}, and PrxL: {proximity_loss} and PL {plausability_loss} and sparl: {sparsity_loss}')\n",
    "    \n",
    "    loss = np.reshape(np.array(yloss + conditional_term*((diversity_loss*diversity_weight)+ (proximity_weight * proximity_loss) +\n",
    "                                    (sparsity_weight * sparsity_loss) +(plausability_loss*plausability_weight))), (-1, 1))\n",
    "    index = np.reshape(np.arange(len(cfs)), (-1, 1))\n",
    "    loss = np.concatenate([index, loss], axis=1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cfs(qi, cfs, features):\n",
    "    \"\"\"\n",
    "    Construct a DataFrame from a list of instances cfs,\n",
    "    filling fields that are the same as in instance qi with '-'.\n",
    "    \n",
    "    Parameters:\n",
    "    qi (dict): An instance represented as a dictionary.\n",
    "    cfs (list of dicts): A list of instances, each represented as a dictionary.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Transformed DataFrame with matching fields replaced by '-'.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(cfs, columns=features)\n",
    "    \n",
    "    # Replace fields that are the same as in qi with '-'\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: '-' if x == qi[df.columns.get_loc(col)] else f'{qi[df.columns.get_loc(col)]}->{x}')\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization for Causal counterfactual generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def generate_single_cf(query_instance, desired_class, adjacency_matrix, causal_order, proximity_weight, sparsity_weight, plausibility_weight, \n",
    "                       diversity_weight, bounds, model, \n",
    "                       categorical_indicator=None, features_order=None, masked_features=None, cfs=[], X=None, init_points=5, n_iter=1000):\n",
    "    \"\"\"\n",
    "    Generate a single counterfactual that minimizes the loss function using Optuna.\n",
    "    \"\"\"\n",
    "    if categorical_indicator is None:\n",
    "        categorical_indicator = [False]*len(bounds)\n",
    "    if features_order is None:\n",
    "        features_order = list(bounds.keys())\n",
    "    if masked_features is None:\n",
    "        masked_features = features_order\n",
    "\n",
    "    def update_masked_features_dict(features_order, masked_features, **params):\n",
    "        for feature, value in zip(features_order, query_instance):\n",
    "            if feature not in masked_features:\n",
    "                params[feature] = value\n",
    "        return params\n",
    "        \n",
    "    def black_box_function(trial):\n",
    "        # Suggest values for each feature based on bounds\n",
    "        kwargs = {}\n",
    "        for i, key in enumerate(features_order):\n",
    "            if categorical_indicator[i]:\n",
    "                kwargs[key] = trial.suggest_categorical(key, list(range(int(bounds[key][0]), int(bounds[key][1])+1)))\n",
    "            else:\n",
    "                kwargs[key] = trial.suggest_uniform(key, bounds[key][0], bounds[key][1])\n",
    "\n",
    "        kwargs = update_masked_features_dict(features_order, masked_features, **kwargs)\n",
    "        #todo rounding\n",
    "        cf = np.array([[int(round(kwargs[key])) if categorical_indicator[i] else kwargs[key] \n",
    "                        for i, key in enumerate(features_order)]]).reshape(1, -1)\n",
    "        \n",
    "        if len(cfs) > 0:\n",
    "            cfst = np.vstack(cfs + [cf])\n",
    "        else:\n",
    "            cfst = cf\n",
    "\n",
    "        loss = compute_loss(model, cf, query_instance, desired_class, adjacency_matrix, \n",
    "                            causal_order, proximity_weight, sparsity_weight, plausibility_weight,\n",
    "                            diversity_weight, pbounds=bounds, features_order=features_order, masked_features=masked_features,\n",
    "                            categorical=categorical_indicator, allcfs=cfst)\n",
    "        # Return the first value in the loss array, Optuna needs a single scalar value to minimize\n",
    "        return -loss[0, 1]\n",
    "\n",
    "    # Initialize Optuna study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "\n",
    "    # Define seen_points set to track uniqueness of points\n",
    "    seen_points = set()\n",
    "\n",
    "    def is_unique_point(point_dict):\n",
    "        point_tuple = tuple(point_dict[key] for key in features_order)\n",
    "        if point_tuple in seen_points:\n",
    "            return False\n",
    "        seen_points.add(point_tuple)\n",
    "        return True\n",
    "    \n",
    "    def clip_values(cf, pbounds):\n",
    "        # Create a new dictionary with clipped values based on the bounds in pbounds\n",
    "        return {feature: np.clip(value, pbounds[feature][0], pbounds[feature][1]) \n",
    "                for feature, value in cf.items()}\n",
    "\n",
    "\n",
    "\n",
    "    sampled_trials = 0\n",
    "    if X is not None:\n",
    "        Xdesired = X[model.predict(X) == desired_class].drop_duplicates()\n",
    "        sample_size = min(int(init_points * 0.5), len(Xdesired))\n",
    "        Xsample = Xdesired.sample(sample_size)\n",
    "        for i, r in Xsample.iterrows():\n",
    "            candidate_point = dict(r[features_order])\n",
    "            if is_unique_point(candidate_point):\n",
    "                trial_params = {key: candidate_point[key] for key in features_order}\n",
    "                study.enqueue_trial(trial_params)\n",
    "                sampled_trials +=1\n",
    "        init_points = max(0, init_points - sampled_trials)\n",
    "        \n",
    "\n",
    "    if init_points > 0:\n",
    "        try:\n",
    "            print(f'EBM random samples... Already sampled {sample_size} from {Xdesired.shape[0]} possible...')\n",
    "            ebcf = EBMCounterOptimizer(model, pd.DataFrame(query_instance.reshape(1, -1), columns=features_order))\n",
    "            total_lists = []\n",
    "            for i in range(min(init_points, 2**len(masked_features))):\n",
    "                num_elements = random.randint(1, len(masked_features))\n",
    "                selected_features = random.sample(masked_features, num_elements)\n",
    "                set_to_check = set(selected_features)\n",
    "                # Check if the set is in the list of sets\n",
    "                found = any(set(item) == set_to_check for item in total_lists)\n",
    "                if found:\n",
    "                    continue\n",
    "                total_lists.append(selected_features)\n",
    "                try:\n",
    "                    _, cf = ebcf.optimize_proba(desired_class, feature_masked=selected_features)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                #check if values of cf are aligned with the ranges, and if not, clip it to the range\n",
    "                cf = clip_values(cf, bounds)\n",
    "\n",
    "                if isinstance(cf, dict):\n",
    "                    cf_dict = cf\n",
    "                elif isinstance(cf, np.ndarray) or isinstance(cf, list):\n",
    "                    cf_dict = {key: cf[i] for i, key in enumerate(features_order)}\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected format for cf returned by optimize_proba.\")\n",
    "\n",
    "                if is_unique_point(cf_dict):\n",
    "                    sampled_trials += 1\n",
    "                    study.enqueue_trial(cf_dict)\n",
    "        except:\n",
    "            print('Resampling failed...')\n",
    "\n",
    "    # Optimize the study with the defined number of iterations\n",
    "    study.optimize(black_box_function, n_trials=n_iter + sampled_trials)\n",
    "\n",
    "    # Extract the best parameters (counterfactual)\n",
    "    best_params = study.best_params\n",
    "    best_params = update_masked_features_dict(features_order, masked_features, **best_params)\n",
    "    #todo rounding should be done to the featoure boundaries, not to nearest integer\n",
    "    best_cf = np.array([[int(round(best_params[key])) if categorical_indicator[i] else best_params[key] \n",
    "                        for i, key in enumerate(features_order)]]).reshape(1, -1)\n",
    "\n",
    "    return best_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cfs(query_instance, desired_class, adjacency_matrix, causal_order, proximity_weight, \n",
    "                 sparsity_weight, plausibility_weight, diversity_weight, bounds,  model, features_order, masked_features=None, categorical_indicator = None, X=None,\n",
    "                 num_cfs=1, init_points=10, n_iter=1000):\n",
    "    \"\"\"\n",
    "    Generate multiple counterfactuals that minimize the loss function using Bayesian Optimization.\n",
    "    \n",
    "    Parameters:\n",
    "        query_instance: The instance to generate counterfactuals for.\n",
    "        desired_class: The target class for the counterfactuals.\n",
    "        adjacency_matrix: The adjacency matrix representing the causal structure.\n",
    "        causal_order: The order of variables in the causal graph.\n",
    "        proximity_weight, sparsity_weight, plausibility_weight: Weights for different loss components.\n",
    "        bounds: The bounds for each feature to search over (dict with feature names as keys and tuple (min, max) as values).\n",
    "        model: The predictive model used to predict class labels.\n",
    "        categorical_indicator: True at the index where the variable should be treated as categorical\n",
    "        num_cfs: The number of counterfactual instances to generate.\n",
    "        init_points: Number of initial points for Bayesian Optimization.\n",
    "        n_iter: Number of iterations for Bayesian Optimization.\n",
    "\n",
    "    Returns:\n",
    "        The generated counterfactuals that minimize the loss function.\n",
    "    \"\"\"\n",
    "    cfs = []\n",
    "    for _ in range(num_cfs):\n",
    "        cf = generate_single_cf(query_instance, desired_class, adjacency_matrix, \n",
    "                                causal_order, proximity_weight, sparsity_weight, \n",
    "                                plausibility_weight, diversity_weight,\n",
    "                                bounds, model,categorical_indicator,features_order, masked_features=masked_features,cfs=cfs,X=X,init_points=init_points, n_iter=n_iter)\n",
    "        cfs.append(cf)\n",
    "\n",
    "    return np.vstack(cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_pbounds(ebm_model, feature_names, features_masked=None):\n",
    "    bonds = dict([[f,(ebm_model.feature_bounds_[i][0],ebm_model.feature_bounds_[i][1])] for i,f in enumerate(feature_names)])\n",
    "    if features_masked is None:\n",
    "        return bonds\n",
    "    else:\n",
    "        return dict([[f,bonds[f]] for f in features_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ccf(explain_instance, model_clf, dataset, desired_class, num_cfs, casual_model, pbounds, as_causal=True, masked_features=None, init_points=500, n_iter=100):\n",
    "    return generate_cfs(explain_instance, desired_class=desired_class, adjacency_matrix=casual_model.adjacency_matrix_, causal_order=casual_model.causal_order_, \n",
    "                               proximity_weight=1, \n",
    "                               sparsity_weight=1, \n",
    "                               categorical_indicator = dataset._categorical_indicator,\n",
    "                               plausibility_weight=int(as_causal), \n",
    "                               diversity_weight = 1,\n",
    "                               bounds=pbounds, \n",
    "                               model=model_clf,\n",
    "                               features_order = dataset.features,\n",
    "                               masked_features = masked_features,\n",
    "                               num_cfs=num_cfs,\n",
    "                               X=dataset.df_train,\n",
    "                               init_points=init_points, \n",
    "                               n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on more datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lingam\n",
    "import time\n",
    "from time import time as tstime\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import warnings\n",
    "import random\n",
    "import logging\n",
    "import signal\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recourse.data import OpenmlData, CustomData, DataRecipe\n",
    "from recourse.model import Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Calculation timed out!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the timeout handler for the SIGALRM signal\n",
    "signal.signal(signal.SIGALRM, timeout_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = './storage/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('openml')\n",
    "logger.handlers.clear()\n",
    "logger.propagate = False\n",
    "logging.getLogger('openml').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2file(output, filepath='progress.txt', clear=False):\n",
    "    with open(filepath, 'w' if clear else 'a') as f:\n",
    "        f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "suite = openml.study.get_suite(99)\n",
    "print(suite)\n",
    "tasks = suite.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ebm_model(dataset, n_jobs=-1, random_state=RANDOM_STATE, interactions=0, debug=False):\n",
    "    model_clf = ExplainableBoostingClassifier(random_state=RANDOM_STATE, n_jobs=-1, feature_types=ds.feature_types, interactions=0)\n",
    "    model_clf.fit(ds.df_train[ds.features], ds.df_train[ds.target])\n",
    "    \n",
    "    y_pred = model_clf.predict(ds.df_test[ds.features])\n",
    "    y_test = ds.df_test[ds.target]\n",
    "    if debug:\n",
    "        print(f\"Model accuracy: {round(accuracy_score(y_test, y_pred),2)}\")\n",
    "    \n",
    "    return model_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_causal_model(dataset):\n",
    "    causal_model = lingam.DirectLiNGAM()\n",
    "    causal_model.fit(ds.df_train[ds.features])\n",
    "    \n",
    "    return causal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCStats:\n",
    "    def __init__(self):\n",
    "        self.stats = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_columns():\n",
    "        return ['model_name','dataset','fidelity','probability','loss','proximity_loss','sparsity_loss','causality_loss','diversity',\n",
    "                'no_ov_cfs','ov_loss','ov_proximity_loss','ov_sparsity_loss','ov_causality_loss','ov_diversity'\n",
    "                ,'execution_time']\n",
    "    \n",
    "    def get_stats_for_dataset(self, name):\n",
    "        datasum = pd.DataFrame(self.stats, columns=self.get_columns())\n",
    "        \n",
    "        return datasum[datasum['dataset']==name].groupby('model_name').mean().to_string(header=True)\n",
    "    \n",
    "    def get_total_stats(self):\n",
    "        return pd.DataFrame(self.stats, columns=self.get_columns())\n",
    "    \n",
    "    def save_total_stats(self, filename):\n",
    "        self.get_total_stats().to_csv(filename, index=False)\n",
    "\n",
    "    def append_stats(self, method, cfs, dataset, explain_instance, model_clf, casual_model, desired_class, pbounds, execution_time):\n",
    "        \"\"\"\n",
    "        Append statistical evaluation metrics to the global `stats` list.\n",
    "\n",
    "        This function evaluates various metrics for a given set of counterfactuals (cfs) and appends the results to a global list named `stats`. The evaluation metrics include accuracy, y-loss, proximity loss, sparsity loss, causal penalty, and diversity loss.\n",
    "\n",
    "        Parameters:\n",
    "        name (str): The name or identifier for the set of counterfactuals being evaluated.\n",
    "        cfs (np.ndarray): An array of counterfactual instances to be evaluated.\n",
    "\n",
    "        Metrics Evaluated:\n",
    "        - Accuracy: The mean accuracy of the model's predictions on the counterfactuals.\n",
    "        - y-Loss: The mean y-loss, which measures the difference between the model's predicted and desired classes.\n",
    "        - Proximity Loss: The mean proximity loss, which measures the distance between the counterfactuals and the original instance being explained.\n",
    "        - Sparsity Loss: The mean sparsity loss, which measures how many features differ between the counterfactuals and the original instance.\n",
    "        - Causal Penalty: The mean causal penalty, which assesses the impact of the counterfactuals on a causal model.\n",
    "        - Diversity: A measure of diversity among the counterfactuals.\n",
    "\n",
    "        The results are appended as a list to the global `stats` list in the following order:\n",
    "        [name, dataset,accuracy, mean_y_loss, mean_proximity_loss, mean_sparsity_loss, mean_causal_penalty, diversity,execution_time]\n",
    "\n",
    "        Returns:\n",
    "        List\n",
    "\n",
    "        Example:\n",
    "        >>> name = \"Counterfactual Set 1\"\n",
    "        >>> cfs = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
    "        >>> append_stats(name, cfs)\n",
    "        \"\"\"\n",
    "        categorical_indicator = dataset._categorical_indicator\n",
    "\n",
    "        record=[]\n",
    "        record.append(method)\n",
    "        record.append(dataset.name)\n",
    "        if cfs is not None:\n",
    "            record.append(np.mean(desired_class==model_clf.predict(cfs)))\n",
    "            record.append(np.mean([p[desired_class] for p in model_clf.predict_proba(cfs)]))\n",
    "            record.append(np.mean([compute_yloss(model_clf,ce, desired_class)[0] for ce in cfs]))\n",
    "            record.append(np.mean([compute_proximity_loss(ce.reshape(1,-1), explain_instance,pbounds=pbounds,features_order=dataset.features,categorical=categorical_indicator) for ce in cfs]))\n",
    "            record.append(np.mean([compute_sparsity_loss(ce.reshape(1,-1),explain_instance) for ce in cfs]))\n",
    "            record.append(np.mean([compute_causal_penalty(ce.reshape(1,-1),casual_model.adjacency_matrix_,casual_model.causal_order_,categorical=categorical_indicator) for ce in cfs]))\n",
    "            record.append(compute_diversity_loss(cfs))\n",
    "            \n",
    "            mask = (desired_class==model_clf.predict(cfs))\n",
    "            ovcfs = [cfs[i] for i in range(len(cfs)) if mask[i] ]\n",
    "            \n",
    "            if len(ovcfs) > 0:\n",
    "                record.append(len(ovcfs))\n",
    "                record.append(np.mean([compute_yloss(model_clf,ce, desired_class)[0] for ce in ovcfs]))\n",
    "                record.append(np.mean([compute_proximity_loss(ce.reshape(1,-1), explain_instance,pbounds=pbounds,features_order=dataset.features,categorical=categorical_indicator) for ce in ovcfs]))\n",
    "                record.append(np.mean([compute_sparsity_loss(ce.reshape(1,-1),explain_instance) for ce in ovcfs]))\n",
    "                record.append(np.mean([compute_causal_penalty(ce.reshape(1,-1),casual_model.adjacency_matrix_,casual_model.causal_order_,categorical=categorical_indicator) for ce in ovcfs]))\n",
    "                record.append(compute_diversity_loss(ovcfs))\n",
    "            else:\n",
    "                record.append(0)\n",
    "                record.append(np.nan)\n",
    "                record.append(np.nan)\n",
    "                record.append(np.nan)\n",
    "                record.append(np.nan)\n",
    "                record.append(np.nan)\n",
    "            \n",
    "            record.append(execution_time)\n",
    "        else:\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "            record.append(np.nan)\n",
    "        \n",
    "        self.stats.append(record)\n",
    "\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import traceback\n",
    "import sys\n",
    "sys.path.append('./recourse/model/LORE')\n",
    "sys.path.append('./recourse/model')\n",
    "from LORE import lore\n",
    "from LORE.neighbor_generator import *\n",
    "from recourse.utils import utils\n",
    "from cfnow import find_tabular\n",
    "from alibi.explainers import CounterfactualProto\n",
    "from alibi.explainers import CEM\n",
    "from alibi.explainers import Counterfactual\n",
    "\n",
    "\n",
    "class ExplainersRegistry:\n",
    "    def __init__(self, model_clf, causal_model, dataset, num_cfs, stats, time_limit=1800, n_iter=100, init_points=500):\n",
    "        self.model_clf = model_clf\n",
    "        self.causal_model = causal_model\n",
    "        self.ds = dataset\n",
    "        self.num_cfs = num_cfs\n",
    "        self.stats = stats\n",
    "        self.time_limit = time_limit\n",
    "        self.n_iter = n_iter\n",
    "        self.init_points = init_points        \n",
    "        \n",
    "        self.pbounds = self.calc_pbounds()\n",
    "        self.desired_class = None\n",
    "\n",
    "        self.recipes = {}\n",
    "        self.register_explainer('CCF', self._method_ccf)\n",
    "        self.register_explainer('CCF-no-causal', self._method_ccf_no_causal)\n",
    "        self.register_explainer('Dice', self._method_dice)\n",
    "        self.register_explainer('Baseline', self._method_baseline)\n",
    "        self.register_explainer('LUX', self._method_lux)\n",
    "        self.register_explainer('LORE', self._method_lore)\n",
    "        self.register_explainer('CEM', self._method_cem)\n",
    "        self.register_explainer('CFProto', self._method_protocf)\n",
    "        self.register_explainer('Wachter', self._method_wachter)\n",
    "        self.register_explainer('cfnow', self._method_cfnow)\n",
    "    \n",
    "        \n",
    "    def _method_baseline(self):\n",
    "        ebcf = EBMCounterOptimizer(self.model_clf, pd.DataFrame(self.explain_instance.reshape(1,-1), columns=self.ds.features))\n",
    "        cfs=[]\n",
    "        for i in range(self.num_cfs):\n",
    "            proba, cf = ebcf.optimize_proba(self.desired_class, feature_masked=self.ds.features)        \n",
    "            cfs.append(cf)\n",
    "        return pd.DataFrame(cfs).values\n",
    "        \n",
    "    def _method_ccf(self):\n",
    "        return run_ccf(self.explain_instance, self.model_clf, self.ds, \n",
    "                       self.desired_class, self.num_cfs, self.causal_model, self.pbounds,\n",
    "                      init_points=self.init_points, n_iter=self.n_iter)\n",
    "        \n",
    "    def _method_ccf_no_causal(self):\n",
    "        return run_ccf(self.explain_instance, self.model_clf, self.ds, \n",
    "                       self.desired_class, self.num_cfs, self.causal_model, self.pbounds, as_causal=False,\n",
    "                      init_points=self.init_points, n_iter=self.n_iter)\n",
    "    \n",
    "    def _method_cem(self):\n",
    "        Xtr = self.ds.df_train[self.ds.features].values\n",
    "        mode = 'PN'  # 'PN' (pertinent negative) or 'PP' (pertinent positive)\n",
    "        shape = (1,) + Xtr.shape[1:]  # instance shape\n",
    "        kappa = .2  # minimum difference needed between the prediction probability for the perturbed instance on the\n",
    "                    # class predicted by the original instance and the max probability on the other classes\n",
    "                    # in order for the first loss term to be minimized\n",
    "        beta = .1  # weight of the L1 loss term\n",
    "        c_init = 10.  # initial weight c of the loss term encouraging to predict a different class (PN) or\n",
    "                      # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "        c_steps = 10  # nb of updates for c\n",
    "        max_iterations = 1000  # nb of iterations per value of c\n",
    "        feature_range = (Xtr.min(axis=0).reshape(shape)-.1,  # feature range for the perturbed instance\n",
    "                         Xtr.max(axis=0).reshape(shape)+.1)  # can be either a float or array of shape (1xfeatures)\n",
    "        clip = (-1000.,1000.)  # gradient clipping\n",
    "        lr_init = 1e-2  # initial learning rate\n",
    "\n",
    "        cem = CEM(self.model_clf.predict_proba, \n",
    "          mode, \n",
    "          shape, \n",
    "          kappa=kappa, \n",
    "          beta=beta, \n",
    "          feature_range=feature_range,\n",
    "          max_iterations=max_iterations, \n",
    "          c_init=c_init, \n",
    "          c_steps=c_steps,\n",
    "          learning_rate_init=lr_init, \n",
    "          clip=clip)\n",
    "\n",
    "        cem.fit(Xtr, no_info_type='median')  # we need to define what feature values contain the least\n",
    "                                                 # info wrt predictions\n",
    "                                                 # here we will naively assume that the feature-wise median\n",
    "                                                 # contains no info; domain knowledge helps!\n",
    "        explanation = cem.explain(self.explain_instance.reshape(1,-1), verbose=False)\n",
    "        if explanation['PN'] is not None:\n",
    "            cfs = [explanation['PN'].ravel() for _ in range(self.num_cfs)]\n",
    "        else:\n",
    "            raise Exception('No CF found')\n",
    "        return cfs\n",
    "\n",
    "    def _method_protocf(self):\n",
    "        Xtr = self.ds.df_train[self.ds.features].values\n",
    "        shape = (1,) + Xtr.shape[1:]\n",
    "        beta = .01\n",
    "        c_init = 1.\n",
    "        c_steps = 5\n",
    "        max_iterations = 500\n",
    "        rng = (-1., 1.)  # scale features between -1 and 1\n",
    "        rng_shape = (1,) + Xtr.shape[1:]\n",
    "        feature_range = (Xtr.min(axis=0).reshape(shape)-.1,  # feature range for the perturbed instance\n",
    "                         Xtr.max(axis=0).reshape(shape)+.1)  # can be either a float or array of shape (1xfeatures)\n",
    "        cat_vars_ord={}\n",
    "        for i,ci in enumerate(self.ds.categorical_indicator):\n",
    "            if ci:\n",
    "                cat_vars_ord[i]=len(np.unique(Xtr[:,i]))\n",
    "        cf = CounterfactualProto(self.model_clf.predict_proba,\n",
    "                 shape,\n",
    "                 beta=beta,\n",
    "                 cat_vars=cat_vars_ord,\n",
    "                 max_iterations=max_iterations,\n",
    "                 feature_range=feature_range,\n",
    "                 c_init=c_init,\n",
    "                 c_steps=c_steps,\n",
    "                 eps=(.01, .01)  # perturbation size for numerical gradients\n",
    "                )\n",
    "        cf.fit(Xtr, d_type='abdm', disc_perc=[25, 50, 75])\n",
    "        explanation = cf.explain(self.explain_instance.reshape(1,-1),\n",
    "                                 target_class=[self.desired_class])\n",
    "        if explanation['cf'] is not None:\n",
    "            cfs = [explanation['cf']['X'].ravel() ]\n",
    "        else:\n",
    "            raise Exception('No CF found')\n",
    "        return cfs\n",
    "        \n",
    "    def _method_wachter(self):\n",
    "        Xtr = self.ds.df_train[self.ds.features].values\n",
    "        shape = (1,) + Xtr.shape[1:]\n",
    "        beta = .01\n",
    "        c_init = 1.\n",
    "        c_steps = 5\n",
    "        max_iterations = 500\n",
    "        rng = (-1., 1.)  # scale features between -1 and 1\n",
    "        rng_shape = (1,) + Xtr.shape[1:]\n",
    "        feature_range = ((np.ones(rng_shape) * rng[0]).astype(np.float32),\n",
    "                         (np.ones(rng_shape) * rng[1]).astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cf = Counterfactual(self.model_clf.predict_proba, shape, distance_fn='l1', target_proba=1.0,\n",
    "                            target_class=int(self.desired_class), max_iter=1000, early_stop=50, lam_init=1e-1,\n",
    "                            max_lam_steps=10, tol=0.05, learning_rate_init=0.1,\n",
    "                            feature_range=feature_range, eps=0.01, init='identity',\n",
    "                            decay=True, write_dir=None, debug=False)\n",
    "        \n",
    "        explanation = cf.explain(self.explain_instance.reshape(1,-1))\n",
    "        if explanation['cf'] is not None:\n",
    "            cfs= [explanation['cf']['X'].ravel()]\n",
    "        else:\n",
    "            raise Exception('No CF found')\n",
    "        return cfs\n",
    "    \n",
    "    def _method_lux(self):\n",
    "        Xtr,_ = train_test_split(self.ds.df_train.copy(), train_size=min(self.ds.df_train.shape[0],2000)) \n",
    "        cols = [f.replace('-','') for f in Xtr.columns]\n",
    "        Xtr.columns =  cols\n",
    "\n",
    "        features = [f for f in cols if f not in ds.target]\n",
    "        explain_instance = self.explain_instance.reshape(1,-1)\n",
    "        lux = LUX(predict_proba=self.model_clf.predict_proba, \n",
    "                  #classifier=model_clf, \n",
    "                  neighborhood_size=0.1)\n",
    "        lux.fit(Xtr[features],Xtr[self.ds.target],instance_to_explain=explain_instance, categorical=self.ds._categorical_indicator, n_jobs=-1)\n",
    "        \n",
    "        _,Xs = train_test_split(Xtr[features],stratify=Xtr[self.ds.target], test_size=1000)\n",
    "        \n",
    "        cf_lux = lux.counterfactual(np.array(explain_instance), Xs[features], counterfactual_representative='nearest', topn=self.num_cfs)\n",
    "        cfs = [np.array(c['counterfactual']) for c in cf_lux]\n",
    "        \n",
    "        return cfs\n",
    "    \n",
    "    def _method_dice(self):\n",
    "        hyperparams = {\"num\": self.num_cfs, \"desired_class\": int(self.desired_class), \"posthoc_sparsity_param\": 0.1}\n",
    "        dice = Dice(self.model_clf, data=self.ds, hyperparams=hyperparams)\n",
    "\n",
    "        return dice.get_counterfactuals(pd.DataFrame([self.explain_instance], columns=self.ds.features)).values\n",
    "    \n",
    "    def _method_cfnow(self):\n",
    "        try:\n",
    "            local_time_limit = int(self.cfgen_time_agg/self.cfgen_methods_no)\n",
    "            if local_time_limit < 20:\n",
    "                # make it reasonable limit for this kind of search algorithm\n",
    "                local_time_limit = 20\n",
    "        except:\n",
    "            local_time_limit=20\n",
    "            \n",
    "        cf_obj = find_tabular(\n",
    "            factual=pd.Series(self.explain_instance),\n",
    "            count_cf=self.num_cfs,\n",
    "            feat_types={i: 'cat' if self.ds.categorical_indicator[i] else 'cont' for i in range(len(self.ds.categorical_indicator))},\n",
    "            model_predict_proba=self.model_clf.predict_proba,\n",
    "            limit_seconds= local_time_limit)\n",
    "        cfs= list(cf_obj.cfs[:self.num_cfs]) if cf_obj.total_cf >= self.num_cfs else list(cf_obj.cfs)\n",
    "        return cfs\n",
    "        \n",
    "    \n",
    "    def _method_lore(self):\n",
    "        Xtr= self.ds.df_train.copy()\n",
    "        cols = [f.replace('-','') for f in Xtr.columns]\n",
    "        Xtr.columns =  cols\n",
    "        features = [f for f in cols if f not in self.ds.target]\n",
    "        \n",
    "        myds = utils.prepare_ds_lore(Xtr,discrete=self.ds.categorical_indicator,class_name=self.ds.target_class)\n",
    "        X_explain = np.concatenate(( self.explain_instance.reshape(1,-1), myds['X']))\n",
    "        try:\n",
    "            exp_LORE, info_LORE = lore.explain(0, X_explain,\n",
    "                                               myds, self.model_clf,\n",
    "                                               ng_function=genetic_neighborhood,\n",
    "                                               discrete_use_probabilities=True,\n",
    "                                               continuous_function_estimation=False,\n",
    "                                               returns_infos=True, path='./recourse/model/LORE/yadt/',\n",
    "                                               sep=';', log=True, depth=100)\n",
    "        except:\n",
    "            #Try numerical only\n",
    "            myds = utils.prepare_ds_lore(Xtr,class_name=self.ds.target_class)\n",
    "            X_explain = np.concatenate(( self.explain_instance.reshape(1,-1), myds['X']))\n",
    "            exp_LORE, info_LORE = lore.explain(0, X_explain,\n",
    "                                               myds, self.model_clf,\n",
    "                                               ng_function=genetic_neighborhood,\n",
    "                                               discrete_use_probabilities=True,\n",
    "                                               continuous_function_estimation=False,\n",
    "                                               returns_infos=True, path='./recourse/model/LORE/yadt/',\n",
    "                                               sep=';', log=True, depth=100)\n",
    "    \n",
    "        query = ' & '.join([f'({str(a)} {str(b)})' if str(a) not in str(b) else f'({b})' for a,b in exp_LORE[1][0].items()])\n",
    "        cfs = Xtr.query(query)\n",
    "        fcfs = cfs.sample(min(len(cfs), self.num_cfs))\n",
    "        return list(fcfs[features].values)\n",
    "    \n",
    "    def register_explainer(self, name, func):\n",
    "        self.recipes.update({name: func})\n",
    "        \n",
    "    def calc_pbounds(self):\n",
    "        return {f: (self.ds.df[f].min(), self.ds.df[f].max()) for f in self.ds.features}\n",
    "        \n",
    "    def set_desired_class(self, explain_instance, desired_class=None):\n",
    "        if desired_class:\n",
    "            self.desired_class = desired_class\n",
    "        else:\n",
    "            self.desired_class = (self.model_clf.predict(explain_instance)[0]+1) % self.ds.df_train[self.ds.target].nunique()\n",
    "        \n",
    "    def run_explainers(self, explain_instance):\n",
    "        self.set_desired_class(explain_instance)\n",
    "        self.explain_instance = explain_instance\n",
    "        self.cfgen_methods_no=0\n",
    "        self.cfgen_time_agg=0\n",
    "        tf.keras.backend.clear_session()\n",
    "        for method_name, method_func in self.recipes.items():\n",
    "            print(f'Running {method_name}...')\n",
    "            try:\n",
    "                ts = tstime()\n",
    "                signal.alarm(self.time_limit)\n",
    "                cfs = method_func()\n",
    "                te = tstime()-ts\n",
    "                self.cfgen_methods_no+=1\n",
    "                self.cfgen_time_agg+=te\n",
    "                self.stats.append_stats(method_name, cfs, self.ds, self.explain_instance, self.model_clf, self.causal_model, int(self.desired_class), self.pbounds, te)\n",
    "                print('Done (in {:.2f}s)'.format(te))                \n",
    "            except TimeoutError:\n",
    "                print('Timeout...')\n",
    "                self.stats.append_stats(method_name, None, self.ds, self.explain_instance, self.model_clf, self.causal_model, int(self.desired_class), self.pbounds, np.nan)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                traceback.print_exc()  # This prints the full traceback\n",
    "                \n",
    "                self.stats.append_stats(method_name, None, self.ds, self.explain_instance, self.model_clf, self.causal_model, int(self.desired_class), self.pbounds, np.nan)\n",
    "            signal.alarm(0)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_or_dump_cached_file(cache_dir, cached_file_name, cached_data=None):\n",
    "    \"\"\"\n",
    "    Load cached data from the specified file, or if cached_data is provided, \n",
    "    dump it into the file in the specified directory. \n",
    "    \n",
    "    If the file does not exist or loading fails, and cached_data is not provided, return None.\n",
    "\n",
    "    Parameters:\n",
    "    cache_dir (str): Directory where the cache file is stored.\n",
    "    cached_file_name (str): Name of the cache file.\n",
    "    cached_data (object, optional): Data to be cached (default is None).\n",
    "\n",
    "    Returns:\n",
    "    object or None: The loaded cached data, or None if loading is not possible and cached_data is None.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    \n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(cache_dir, cached_file_name)\n",
    "\n",
    "    # If cached_data is provided, dump it into the file\n",
    "    if cached_data is not None:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            pickle.dump(cached_data, file)\n",
    "        return cached_data\n",
    "\n",
    "    # Otherwise, try loading the cached data\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                return pickle.load(file)\n",
    "        except (pickle.UnpicklingError, EOFError, FileNotFoundError):\n",
    "            return None\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = CCStats()\n",
    "log2file('', clear=True)\n",
    "SAMPLE_SIZE = 10\n",
    "NUM_CFS = 3\n",
    "init_points = 200\n",
    "n_iter = int(0.2*init_points)\n",
    "\n",
    "# Set the time limit in seconds for a calculation of one cfs\n",
    "time_limit = 60*30  # seconds (10 minutes)\n",
    "model_time_limit = 3600 # hour\n",
    "\n",
    "use_suite = False\n",
    "\n",
    "if not use_suite:\n",
    "    all_datasets = openml.datasets.list_datasets(output_format='dataframe')\n",
    "    classification_datasets = all_datasets[(all_datasets['NumberOfClasses']>1) & (all_datasets['NumberOfInstances']>2000) & \n",
    "                                           (all_datasets['NumberOfMissingValues']==0) & \n",
    "                                           #(all_datasets['NumberOfInstances']< 10000)& \n",
    "                                           (all_datasets['NumberOfFeatures'] < 30)].drop_duplicates(subset=['name'])\n",
    "    classification_datasets['name'] = classification_datasets['name'].apply(lambda x: x.split('_')[0])\n",
    "    classification_datasets=classification_datasets.drop_duplicates(subset=['name'])\n",
    "    \n",
    "    tasks = classification_datasets['did']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenML datasets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "cache_dir = './cache/'\n",
    "for task_id in tasks:\n",
    "    try:\n",
    "        ds = load_or_dump_cached_file(cache_dir, f'dataset-{task_id}.pkl', cached_data=None)\n",
    "        if ds is None:\n",
    "            ds = OpenmlData(openml.datasets.get_dataset(task_id))\n",
    "            load_or_dump_cached_file(cache_dir, f'dataset-{task_id}.pkl', cached_data=ds)\n",
    "            \n",
    "        log2file(f'Processing ID: {task_id} for name: {ds.name}\\n')\n",
    "        print(f'{\"*\" * 50} {ds.name} {\"*\" * 50}')\n",
    "\n",
    "        print('Training classification model...', end='')\n",
    "        model_clf = load_or_dump_cached_file(cache_dir, f'model_clf-{task_id}.pkl', cached_data=None)\n",
    "        if model_clf is None:\n",
    "            model_clf = train_ebm_model(ds, debug=True)\n",
    "            load_or_dump_cached_file(cache_dir, f'model_clf-{task_id}.pkl', cached_data=model_clf)  \n",
    "        print('Done')\n",
    "\n",
    "        try:\n",
    "            signal.alarm(model_time_limit) # one hour for causal model\n",
    "            print('Training casual model...', end='')\n",
    "            \n",
    "            causal_model = load_or_dump_cached_file(cache_dir, f'causal_model-{task_id}.pkl', cached_data=None)\n",
    "            if causal_model is None:\n",
    "                causal_model = train_causal_model(ds)\n",
    "                load_or_dump_cached_file(cache_dir, f'causal_model-{task_id}.pkl', cached_data=causal_model)  \n",
    "            print('Done.')\n",
    "            signal.alarm(0)\n",
    "        except TimeoutError:\n",
    "            print('Timeout, aborting, moving to another dataset...')\n",
    "            continue\n",
    "\n",
    "        explainers_registry = ExplainersRegistry(model_clf, causal_model, ds, NUM_CFS, stats, time_limit,\n",
    "                                                init_points = init_points, n_iter = n_iter)\n",
    "        print('Calculating counterfactuals...')\n",
    "        for explain_instance in ds.sample_test(SAMPLE_SIZE):\n",
    "            explainers_registry.run_explainers(explain_instance)\n",
    "        log2file(stats.get_stats_for_dataset(ds.name))\n",
    "    except Exception as e:\n",
    "        # Capture the traceback as a string\n",
    "        stack_trace = traceback.format_exc()\n",
    "        log2file(stack_trace)\n",
    "        print(stack_trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf-pub-lux",
   "language": "python",
   "name": "cf-env-pub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
